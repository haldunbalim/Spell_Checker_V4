{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from speller import spell\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, Dropout, recurrent, Embedding\n",
    "from keras.callbacks import Callback\n",
    "from keras import  optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "FILE_FOLDER_NAME = \"text files/\"\n",
    "SPELL_INDEX_FILE = FILE_FOLDER_NAME+\"spell_index.txt\"\n",
    "WORDS_FILE = FILE_FOLDER_NAME+\"full_short.txt\"\n",
    "MAXIMUM_LEN = 10\n",
    "LEARNING_RATE=0.001\n",
    "\n",
    "RANDOM_RATE = 0.7\n",
    "RANDOM_TIMES = 2\n",
    "\n",
    "AMOUNT_OF_DROPOUT = 0.2\n",
    "INITIALIZATION = \"he_normal\"\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_ITERATIONS = 1\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 1000\n",
    "STEPS_PER_EPOCH = 100\n",
    "NUM_SAMPLES_ON_CALLBACK=5\n",
    "NUM_INPUT_LAYERS=1\n",
    "NUM_OUTPUT_LAYERS=1\n",
    "\n",
    "def shuffle_word(word):\n",
    "    r= np.random.randint(len(word)-1)\n",
    "    d=word[0:r]\n",
    "    d+=word[r+1]\n",
    "    d+=word[r]\n",
    "    d+=word[r+2:len(word)]\n",
    "    return d\n",
    "\n",
    "def remove_char(word):\n",
    "    r= np.random.randint(1,len(word))\n",
    "    d=word[0:r]\n",
    "    d+=word[r+1:len(word)]\n",
    "    return d\n",
    "\n",
    "def add_char(word):\n",
    "    r= np.random.randint(1,len(word))\n",
    "    random_char=chr(np.random.randint(ord('a'),ord('z')+1))\n",
    "    d=word[0:r]\n",
    "    d+=random_char\n",
    "    d+=word[r:len(word)]\n",
    "    return d\n",
    "\n",
    "\n",
    "def typo_generator(s,random_rate=0.6,times=0):\n",
    "    d= \"\"\n",
    "    for word in s.split():\n",
    "        r= np.random.rand()\n",
    "        if(r>1-random_rate and len(word)>2):\n",
    "            r=np.random.randint(0,3)\n",
    "            if(r==1):\n",
    "                d+=shuffle_word(word)+' '\n",
    "            elif(r==2):\n",
    "                d+=add_char(word)+' '\n",
    "            else:\n",
    "                d+=remove_char(word)+' '\n",
    "        else:\n",
    "            d+=word+' '\n",
    "    if times==0:\n",
    "        return d[:-1]\n",
    "    else:\n",
    "        return typo_generator(d[:-1],random_rate,times-1)\n",
    "\n",
    "\n",
    "class spell_index:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ind_to_spell = {}\n",
    "        self.spell_to_ind = {}\n",
    "        self.fill_maps()\n",
    "        self.num_spells = len(self.ind_to_spell)\n",
    "\n",
    "    def fill_maps(self):\n",
    "        with open(SPELL_INDEX_FILE, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            shuffle(lines)\n",
    "            for line in lines:\n",
    "                temp = line.split()\n",
    "                spell = temp[0]\n",
    "                index = int(temp[1])\n",
    "                self.ind_to_spell[index] = spell\n",
    "                self.spell_to_ind[spell] = index\n",
    "\n",
    "    def get_index(self, spell):\n",
    "        return self.spell_to_ind[spell]\n",
    "\n",
    "    def get_spell(self, index):\n",
    "        return self.ind_to_spell[index]\n",
    "\n",
    "\n",
    "\n",
    "def _vectorize(word,padding=False):\n",
    "    current=[]\n",
    "    if padding:\n",
    "        current.append(indexer.get_index(\"GO\"))\n",
    "        for s in spell(word):\n",
    "            try:\n",
    "                current.append(indexer.get_index(s))\n",
    "            except:\n",
    "                current.append(indexer.get_index(\"UNKNOWN\"))\n",
    "        current.append(indexer.get_index(\"END\"))\n",
    "        return current\n",
    "    else:\n",
    "        for s in spell(word):\n",
    "            try:\n",
    "                current.append(indexer.get_index(s))\n",
    "            except:\n",
    "                current.append(indexer.get_index(\"UNKNOWN\"))\n",
    "        return current\n",
    "\n",
    "def vectorize(word):\n",
    "    vec=_vectorize(word)\n",
    "    result=np.zeros([MAXIMUM_LEN,indexer.num_spells])\n",
    "    if len(vec) <= MAXIMUM_LEN:\n",
    "        for ind,el in enumerate(vec):\n",
    "            result[ind][el]=1\n",
    "        return result\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "def devectorize(vector, one_hot=False,padding=False):\n",
    "    if one_hot:\n",
    "        1+1\n",
    "    if padding:\n",
    "        1+1\n",
    "\n",
    "    st=\"\"\n",
    "\n",
    "    for i in range(MAXIMUM_LEN):\n",
    "        current = np.argmax(vector[i, :])\n",
    "\n",
    "        if current==0:\n",
    "            return st\n",
    "\n",
    "        try:\n",
    "            current=indexer.get_spell(current)\n",
    "        except:\n",
    "            current=\"**\"\n",
    "\n",
    "        if current==\"UNKNOWN\":\n",
    "            st+=\"**\"\n",
    "        else:\n",
    "            st += current\n",
    "    return st\n",
    "\n",
    "\n",
    "def print_random_predictions(model):\n",
    "    \"\"\"Select 10 samples from the validation set at random so we can visualize errors\"\"\"\n",
    "    print()\n",
    "    for _ in range(NUM_SAMPLES_ON_CALLBACK):\n",
    "        Q,A,V= ds.random_sample()\n",
    "        V=np.array([V[0]])\n",
    "        preds = model.predict(V)\n",
    "\n",
    "        guess=devectorize(preds[0])\n",
    "\n",
    "        print('Q:', Q)\n",
    "        print('A:', A)\n",
    "        print(\"P:\", guess)\n",
    "        print('---')\n",
    "    print()\n",
    "\n",
    "class OnEpochEndCallback(Callback):\n",
    "    \"\"\"Execute this every end of epoch\"\"\"\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"On Epoch end - do some stats\"\"\"\n",
    "        print_random_predictions(self.model)\n",
    "        self.model.save(SAVED_MODEL_FILE_NAME.format(epoch))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class dataset:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.fill_data()\n",
    "        print(\"Data is read\")\n",
    "\n",
    "        self.data_vectors = []\n",
    "        self.fill_data_vectors()\n",
    "        print(\"Data is vectorized\")\n",
    "\n",
    "        self.num_samples = len(self.data)\n",
    "        self.current = 0\n",
    "        print(\"Dataset is created\")\n",
    "\n",
    "    def fill_data(self):\n",
    "        with open(WORDS_FILE) as file:\n",
    "            for line in file.readlines():\n",
    "                word = line.split()[0]\n",
    "                self.data.append((typo_generator(word, random_rate=RANDOM_RATE, times=RANDOM_TIMES), word))\n",
    "        shuffle(self.data)\n",
    "\n",
    "    def fill_data_vectors(self):\n",
    "        long_ls=[]\n",
    "\n",
    "        for ind, el in enumerate(self.data):\n",
    "            vec_x = vectorize(el[0])\n",
    "            vec_y = vectorize(el[1])\n",
    "            if vec_x.any() != 0 or vec_y.any() != 0:\n",
    "                self.data_vectors.append((vec_x, vec_y))\n",
    "            else:\n",
    "                long_ls.append(ind)\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        if self.current + batch_size >= self.num_samples:\n",
    "            self.reset()\n",
    "        self.current += batch_size\n",
    "        ls = self.data_vectors[self.current - batch_size:self.current]\n",
    "        return np.array([el[0] for el in ls]), np.array([el[1] for el in ls])\n",
    "\n",
    "    def reset(self):\n",
    "        self.current = 0\n",
    "        self.fill_data()\n",
    "        self.fill_data_vectors()\n",
    "\n",
    "    def generator(self):\n",
    "        while True:\n",
    "            yield self.next_batch(BATCH_SIZE)\n",
    "\n",
    "    def random_sample(self):\n",
    "        ind = np.random.randint(0, len(self.data))\n",
    "        a=self.data[ind][0]\n",
    "        b=self.data[ind][1]\n",
    "        c=self.data_vectors[ind]\n",
    "        return a, b, c\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def iterate_training(model, X_train, y_train):\n",
    "    \"\"\"Iterative Training\"\"\"\n",
    "    # Train the model each generation and show predictions against the validation dataset\n",
    "    for iteration in range(0, NUM_ITERATIONS):\n",
    "        print()\n",
    "        print('-' * 50)\n",
    "        print('Iteration', iteration)\n",
    "        model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS)\n",
    "\n",
    "\n",
    "def generate_model():\n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(NUM_INPUT_LAYERS):\n",
    "        model.add(recurrent.GRU(HIDDEN_SIZE, input_shape=(None, indexer.num_spells),\n",
    "                                kernel_initializer = INITIALIZATION, return_sequences=i+1<NUM_INPUT_LAYERS))\n",
    "        model.add(Dropout(AMOUNT_OF_DROPOUT))\n",
    "\n",
    "    model.add(RepeatVector(MAXIMUM_LEN))\n",
    "\n",
    "    for i in range(NUM_OUTPUT_LAYERS):\n",
    "        model.add(recurrent.GRU(HIDDEN_SIZE,return_sequences=True,kernel_initializer=INITIALIZATION))\n",
    "        model.add(Dropout(AMOUNT_OF_DROPOUT))\n",
    "\n",
    "    model.add(TimeDistributed(Dense(indexer.num_spells, kernel_initializer=INITIALIZATION)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    optimizer = optimizers.Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def network(x):\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ON_EPOCH_END_CALLBACK = OnEpochEndCallback()\n",
    "    indexer = spell_index()\n",
    "    ds = dataset()\n",
    "    train_speller(LOAD_MODEL_FILE_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
